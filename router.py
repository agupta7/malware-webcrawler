import web
from WebCrawller import WebCrawller

urls = (
    '/', 'Crawl',
    '/crawl', 'Crawl'
)

class Crawl:
    def GET(self):
        web.header('Content-Type', 'application/json')
        queryParams = web.input(maxDepth=3, root='auburn.edu')
        # TODO : sanitize input parameters before handing them off to the crawler
        data = WebCrawller.crawlIDS(queryParams.root, int(queryParams.maxDepth))
        print 'Urls successfully analyzed: ' + str(data['urls'])
        print 'Time taken (milliseconds): ' + str(data['time'])
        print 'Links avoid due to cycles: ' + str(data['avoided'])
        print 'Problem links: ' + str(data['errors'])
        print 'Sum of unigram: ' + str(data['unigramSum'])
        return data['json']

if __name__ == "__main__":
    app = web.application(urls, globals())
    # TODO: To check if the port is available before trying to connect with the default port and give a
    # proper error message to the user
    app.run()
