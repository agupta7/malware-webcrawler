import numpy as np  
import csv
import operator
import math
#################### load Data from csv file 
def Load_Data  (filename):
	X = []
	Y = []
	with open (filename, "rb") as file:
		reader = csv.reader (file)
		for line in reader:
			Y.append (float(line[1]))
			line.pop(0); line.pop(0) ;	
			X.append (np.array(line,dtype = np.double))

	print "Data loaded with shape: ",np.shape(X),
	#count positives and negatives ! 
	pos = Y.count(1)	
	neg = Y.count (-1)	
	Break = len(X) - min (neg,pos)*2 # find number of majority label instances 

	print " with pos: %d and neg: %d"%(pos,neg) # more is the class of majority instances
	if neg > pos:	more = -1
	elif pos>neg:   more = 1 
	else :		return X,Y

	count = 0 
	'''
	indices = [] # insert indices from last to first to make it easier to delete from array
	for indx in range (len(Y)-1,0,-1):
		if Y[indx] == more and  all (X[indx] ==0 ):
			X.pop(indx);Y.pop(indx)
			count += 1 
	indices = [] # insert indices from last to first to make it easier to delet$
        for indx in range (len(Y)-1,0,-1):
                if Y[indx] == more:
                        X.pop(indx);Y.pop(indx)
			count += 1
		if count == Break:
			break 
	for indx in range (len(Y)-1,0,-1):
                if all ( X[indx] == 0):
                        X.pop(indx);Y.pop(indx)
	'''
        count = 0 ;
        for i in range (Break):
                indx = Y.index (more)
                X.pop(indx);Y.pop(indx)
                count += 1
		if count == Break:
			break 
        pos = Y.count(1)
        neg = Y.count (-1)
        print " with pos: %d and neg: %d"%(pos,neg) # more is the class of majority instances
	print "data's new shape is: ",np.shape (X)

	X = np.array(X)
	Y = np.array(Y)
	return X , Y

##################### this function computes the accuracy of the model 
def Accuracy (output, doutput):
	Acc = 0 ; FP = 0 ; FN = 0 ; TP = 0 ; TN = 0 ; unkn = 0  
	for indx in range (len(output)):
		if output[indx] == 0:
			unkn += 1
		elif doutput[indx] == 1: 
			if output[indx] > 0:
				Acc += 1 
				TP += 1 
			elif output[indx] < 0:
				FN += 1 
		elif doutput[indx] == -1:
			if output[indx] < 0:
				Acc += 1
				TN += 1
			elif output[indx] > 0:
				FP += 1
#	print "UNKOWNS: ", unkn
	Acc = Acc*100.0/len(output)
#	print "Accuracy: %f FP: %d FN: %d TP: %d TN: %d"%(Acc,FP,FN,TP,TN)
	return Acc , FP, FN , TP , TN 

####################  this function returns the std for a single data instance 
def Std (instance):
	mean = sum(instance)/float(len(instance))
	dif_sqr = [math.pow(feature-mean,2) for feature in instance ]
	dif_mean =  sum(dif_sqr)/float(len(dif_sqr))	
	std = math.sqrt(dif_mean)
	return std 

##################### random shuffel for samples
def shuffle_in_unison_scary(a, b):
        rng_state = np.random.get_state()
        np.random.shuffle(a)
        np.random.set_state(rng_state)
        np.random.shuffle(b)
        print "Shuffled"
        return a,b

#################### split data into training and testing, given the test data ratio
def split(ratio_tst, X, Y):
        Xtest, X = np.split(X, [ratio_tst*len(Y)] )
        Ytest, Y = np.split(Y, [ratio_tst*len(Y)] )
        print "splitted"
        return Xtest,X,Ytest,Y


###################### F-score feature Selection function 
def F_score (X):
	FSV = []
	Avg_x =  np.sum (X, axis = 0)/len(X)
	for feat in range (len(X[0])):
		Avg_pos = 0 ; n_pos = 0 ;
		list = [] 
		for indx in range (len(X)):
			if X[indx][feat] >= 0:
				Avg_pos += X[indx][feat]
				n_pos += 1	
		list.append (Avg_pos/n_pos) ;	list.append (n_pos) ;	
		FSV.append (list)

	sum_dif = [] 
	for feat in range (len(X[0])):
                listpos = [] 
                for indx in range (len(X)):
			if X[indx][feat] >= 0:
				listpos.append ( math.pow( X[indx][feat] - FSV[feat][1], 2) )
		sum_dif.append( sum (listpos))

	F = [] 
	for feat in range (len(X[0])):
		a = math.pow(Avg_x[feat],2)
		b = ( 1.0/(FSV[feat][1]-1) )*sum_dif[feat] 
		F.append(a/b)
	
	#for item in F:
	#	print item," ",

from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectFromModel
from sklearn.svm import LinearSVC

def Variance_Feature_Select(X):
        sel = VarianceThreshold(threshold=(.85 * (1 - .85)))
        X = sel.fit_transform(X)
        print "featured Extracted: ",X.shape
        return X
		 	
def SVM_Feature_Select(X,Y,Xtest,Ytest, c):
	lsvc = LinearSVC(C = c , penalty="l1", dual=False).fit(X,Y)
	#lsvc.fit (X,Y)
#	score = lsvc.score (Xtest,Ytest)
	
#        model= SelectFromModel(lsvc , prefit=True)
#        X = model.transform(X)
#        print "featured Extracted: ",X.shape
        return lsvc # X

def Normalization (X):
	print "Normalizing Data:\n"
	Mean = np.zeros(len(X[0]) ,dtype=float)
	Dev = np.zeros(len(X[0]) ,dtype=float)
	
	# normalizing on each feature
	for i in range (len(X[0])):
                Mean[i] = np.mean (X[:,i])
		Dev[i] =  Std(X[:,i])
	X = ( X - Mean)/Dev     # Normalizing the data
	print "MEAN: ", Mean 
	print "---------------------------------"
	print "DEV: ", Dev
	return X 

def standarize (X):
	X_ =  []
	for item in X:
                norm = np.square(item)
                norm = np.sqrt(np.sum (norm))
		if norm !=0:
                	X_.append (item/norm)#item = item/norm
		else:
			X_.append (item)

        return np.array(X_)	

def Normalization_min_max(X):
	x_min = np.zeros(len(X[0]) ,dtype=float)
        x_max= np.zeros(len(X[0]) ,dtype=float)
	
	for i in range( len(X[0])):
		x_min[i] = min (X[:,i])
		x_max[i] = max (X[:,i])

	X = (X - x_min)/(x_max - x_min)
	return X 

from neupy import algorithms, estimators, environment
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

'''
def GRNN_train (X,Y,test, sigma):
        num_train_inst = len(X)
        num_features = len(X[0])
        S1 = np.zeros(len(test))
        S2 = np.zeros(len(test))
#       sigma =  0.11853 #1.41

#       X = add_dimension (X)
#       test = add_dimension (test)     
#       print "SHAPES:, ",np.shape(X), np.shape(test)
        for X_indx , instance in enumerate(X):
#               sigma = general.Std (instance)#calling the function to compute std for current instance
                instance = np.array(instance)
                Diff_sqr_sum = np.zeros(len(test))
                for tst_indx, t_inst in enumerate(test): # computing the sum of sqaure differences for all instances
                        diff= instance - np.array(t_inst)# fi differences
                        diff  *= diff                   # diff squared
                        Diff_sqr_sum[tst_indx] = sum (diff)# sum the differences for each test instance 

                hft_qi = (-0.5*Diff_sqr_sum)/(sigma*sigma) # g = - norm^2/2*sigma^2
                hft_qi = np.exp(hft_qi)         # hf_i = exp(g )

                S1 +=hft_qi # Sum( hf_i (t_q , t_i) )
                S2 += hft_qi*Y[X_indx] # Sum( hf_i (t_q , t_i) )*d_i

        Output = S2/S1

#       print "ouptut: ", Output
        return Output

def K_Folds (K, X,Y, sigma):
#       print "K fold validation with K= %d"%K
        Range = len(X)/K
        cnt = 1 ;
        GRNN_acc = 0
        GRNN_acc1 = 0
        GRNN_acc2 = 0
        FP = 0;        FN = 0
	FP1 = 0;        FN1 = 0
	FP2 = 0;        FN2 = 0
        for indx in range (0,len(X) , Range):
#               print "Test fold num %d "%(cnt),
                Xtest = np.array(X[indx:indx+Range])
                Ytest =  np.array(Y[indx:indx+Range])
                Xtrain = np.concatenate (( X[0:indx], X[indx+Range:len(X)] ))
                Ytrain =np.concatenate ((Y[0:indx],Y[indx+Range:len(Y)]))
#               print " test Size: ", np.shape(test_D), " train size: ",np.shape(train_D)               
		
		# predifined GRNN 
		NN  = algorithms.GRNN (std=0.1,verbose=False)
        	model = NN.train (Xtrain,Ytrain)
		# ADA
		dt =  DecisionTreeClassifier() 
       		bdt =  AdaBoostClassifier(base_estimator = dt,n_estimators=200, learning_rate=5)
        	ADA = bdt.fit (Xtrain,Ytrain)

        	out1 = []; out2 = []
	        for item in Xtest:
#        	        out1.append (NN.predict ([item]) )
                	out2.append (bdt.predict ([item]) )

  	        out1 = np.array(out1)   ;#out1 = out1.reshape(len(out1),1)
               	acc, fp, fn = Accuracy(out1,Ytest)

                FP1 += fp        ; FN1 += fn;       GRNN_acc1 += acc;

  	        out2 = np.array(out2)   ;out2 = out2.reshape(len(out2),1)
                acc, fp, fn = Accuracy(out2,Ytest)
                FP2 += fp        ; FN2 += fn;       GRNN_acc2 += acc;
		
		output = GRNN_train(Xtrain,Ytrain,Xtest, sigma)
                acc, fp, fn = Accuracy(output,Ytest)
                FP += fp        ; FN += fn;       GRNN_acc += acc;

                cnt+=1
	
	print "Acuuracy: %f SUM OF FP %d , FN %d" %(GRNN_acc1/K,FP1,FN1)
       	print "Acuuracy: %f SUM OF FP %d , FN %d" %(GRNN_acc2/K,FP2,FN2)
	print "Acuuracy: %f SUM OF FP %d , FN %d" %(GRNN_acc/K,FP,FN)
        return 0,0,0#GRNN_acc/K , FP, FN
'''
def Main():
	X, Y = Load_Data ("malware_dataset.csv")
	X,Y = shuffle_in_unison_scary (X,Y)	
	X = Normalization_min_max (X)
#	sigma= 0.25798202427
	sigma = 0.224301902592
	Avg_acc , Avg_FP, Avg_FN = K_Folds ( len(X), X,Y ,sigma)
	'''
	Xtest,X,Ytest,Y= split (0.25,X,Y)
	NN  = algorithms.GRNN (std=0.1,verbose=False)
	model = NN.train (X,Y)
	bdt =  AdaBoostClassifier(SVM_Feature_Select (X,Y,Xtest,Ytest, 0.1),
                         algorithm="SAMME",
                         n_estimators=200)	
	ADA = bdt.fit (X,Y)
	out1 = []; out2 = []
        for item in Xtest:
                out1.append (NN.predict ([item]) )
		out2.append (bdt.predict ([item]) )

        out1 = np.array(out1)	;out1 = out1.reshape(len(out1),1)
        out2 = np.array(out2)	;out2 = out2.reshape(len(out2),1)
        
	score1 = Accuracy ( out1, Ytest)
	score2 = Accuracy (out2, Ytest)
	
#	lsvc= 	SVM_Feature_Select (X,Y,Xtest,Ytest, 0.1)
#	score = lsvc.score (Xtest,Ytest)
	print "GRNN", score1, "ADA Boost", score2
	'''
#Main()
