import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import random
import general

def Train (X,Y,lr, k , hidden_neurons, emax , itermax, Beta, lamda, decay):
	Y = Y.reshape (len(Y),1)
	IP1 = np.append(X,np.ones((len(X),1)),1)
	W1 = np.random.rand(hidden_neurons, len(IP1[0])) - 0.5
	W2 = np.random.rand(1,hidden_neurons+1)  - 0.5

	Error_Curve = [] 
	for iter in range (itermax):
		for indx, inst in enumerate(IP1):
			inst = np.array(inst).reshape(len(inst),1)
	# forward propagation first layer
			NET1 = np.dot(W1,inst).transpose() 
       		        Act_1 = np.tanh(k*NET1)#2.0/(1 + np.exp(-k*NET1))-1
        	        F1_prime = k* (1- Act_1*Act_1 )
    	# forward propagation second layer
	                IP2 = np.append(Act_1,np.ones([1,1]),1)
        	        NET2 = np.dot(IP2,  W2.transpose())
        	        Act_2 =  np.tanh(k*NET2) #2.0/(1 + np.exp(-k2*NET2)) -1
        	        F2_prime = k*(1- Act_2*Act_2)
	
	# backward propagation second layer
        	        Error2 = np.subtract(Y[indx], Act_2) 
      		        Delta2 = Error2*F2_prime
	# backward propagation firstlayer
	                Error1 =  np.dot(Delta2,W2)[:, 0:hidden_neurons] # eliminate w0			
      		        Delta1 = Error1*F1_prime

                	DW2 = lr* np.dot(np.transpose(Delta2),IP2)
         	        DW1 = lr* np.dot(inst,Delta1).transpose()

        	        ee = sum (sum ( Error2*Error2)) # compute error
			# update weights

	                if not (indx==0 and iter ==0) :
        	# use momentum to converge faster.
                	        W2 += Beta*DW2 + (1-Beta)*Prev_DW2
                	        W1 += Beta*DW1 + (1-Beta)*Prev_DW1

			else:
				W2 += DW2	
                       		W1 += DW1
			
			Prev_DW1 = DW1 ; Prev_DW2 = DW2 ; Prev_ee = ee 

		'''
	       # Variable Learning Rat
	                        inc = 100.0*(ee - Prev_ee)/Prev_ee
        	                if inc >= 5.0:
                	                lr *= (1.0 / (1.0 + decay*iter))
                	                inc *= -1
          		        if inc >= 5.0:
                      		         lr *=  (1.0 + decay*iter)

		'''
		Error_Curve.append (ee)
		if ee < emax:
			break
		
	return W1,W2,Error_Curve

def predict ( test,W1,W2,k):

        IP1 = np.append(test,np.ones([len(test),1]),1)
        NET1 =  np.dot(IP1, W1.transpose() )
        Activation_1 = np.tanh(k*NET1)#2.0/(1 + np.exp(-k1*NET1)) -1

        IP2 = np.append(Activation_1,np.ones([len(Activation_1),1]),1)
        NET2 = np.dot(IP2,  W2.transpose())
        output= np.tanh(k*NET2)#2.0/(1 + np.exp(-k2*NET2)) - 2

        return output


def K_Folds (K, X,Y,lr,k,hidden_neurons,emax,itermax,beta,lamda,decay ):
#       print "K fold validation with K= %d"%K
        Range = len(X)//K
        cnt = 1 ;
        GRNN_acc = 0;	FP = 0
        FN = 0;	TP = 0 ; TN = 0

        for indx in range (0,len(X) , Range):
                print "Test fold num %d "%(cnt),
		# create your folds. 
                test_D = np.array(X[indx:indx+Range])
                test_L =  np.array(Y[indx:indx+Range])
                train_D = np.concatenate (( X[0:indx], X[indx+Range:len(X)] ))
                train_L =np.concatenate ((Y[0:indx],Y[indx+Range:len(Y)]))
		# train
		print "testing data has pos: ",np.count_nonzero(test_L==1), " neg: ",np.count_nonzero(test_L== -1) 
                W1,W2,Error_Curve = Train(train_D,train_L,lr,k,hidden_neurons,emax,itermax,beta,lamda,decay)
#		print 
#		print "Erorr Curve is as follows: ", Error_Curve
#		print
		# predict & evaluate 
#		output = predict (train_D,W1,W2,k)
#               acc, fp, fn, tp , tn  = general.Accuracy(output,train_L)
#		print "Training Accuracy:%f FP:%d FN:%d TP:%d TN:%d"%(acc,fp,fn,tp,tn)

                output = predict (test_D,W1,W2,k)        
	        acc, fp, fn, tp , tn  = general.Accuracy(output,test_L)
		print "Testing Accuracy:%f FP:%d FN:%d TP:%d TN:%d"%(acc,fp,fn,tp,tn)
                FP += fp        ; FN += fn;       GRNN_acc += acc;
                TP += tp ; TN += tn
                cnt+=1
                print

        #print "Accuracy: %f  FP: %d , FN: %d, TP: %d , TN %d"%(GRNN_acc/K,FP,FN,TP,TN)
        return GRNN_acc/K, FP, FN, TP , TN

import cPickle 

def   General_Predict (test):
	with open ("W1","rb") as file:
                W1 = cPickle.load (file)

        with open ("W2","rb") as file:
                W2 = cPickle.load (file)

        k =1 ; 

	test = np.array(general.standarize (test) ) 
	output = predict (test,W1,W2,k)
	return output 


def Main ():
	X,Y = general.Load_Data("malware_dataset.csv")
	X,Y = general.Balance_Data (X,Y)
        X,Y = general.shuffle_in_unison_scary (X,Y)
#        X = general.Normalization_min_max (X)
	X = general.standarize(X)
	# parameters 
	lr = 0.01423 ; hidden_neurons = 1 ; emax = 0.001 ; K = 3 #len(X)
	beta = 0.815432 ; decay = 0.0001 ; k = 1;  lamda = 1 ; itermax = 100
#	lr = 0.001
	acc, FP,FN,TP,TN = K_Folds(K, X,Y,lr,k,hidden_neurons,emax,itermax,beta,lamda,decay )

	print "\nFinal: acc: %f FP: %d FN: %d TP: %d TN: %d"%(acc, FP,FN,TP,TN) 
	print 
     

	W1,W2,Error_Curve = Train(X,Y,lr,k,hidden_neurons,emax,itermax,beta, lamda,decay)
	
	print "\nstoring NN Model: "

        with open ("Params", "wb") as file:
                cPickle.dump ([lr,k,hidden_neurons,emax,itermax,beta, lamda,decay],file)

        with open ("W1", "wb")as file:
                cPickle.dump (W1,file)

        with open ("W2", "wb")as file:
                cPickle.dump (W2,file)
        print "DONE ! "

#Main () 


test = [ [0.0612932981329, 0.000731575431789, 0.0351474283533, 0.000413499157098, 0.000222653392283, 0.000143134323611,
                 0.000922421196603, 0.010846400967, 0.00547091192468, 0.00547091192468, 0.000111326696142, 0.00236966824645,
                 0.00378510766882, 0.0229651070327, 0.0144088552435, 0.034209103343, 0.00448487547314, 0.00995578739782,
                 0.00597983396418, 0.00341931995292, 0.00332389707052, 0.00295810935462, 0.00208339959922, 0.00214701485416,
                0.0030376284233, 0.00186074620694, 0.00736346575909, 0.00656827507236, 0.0291994020166, 0.0246191036611,
                0.0290403638793, 0.000731575431789, 0.0, 0.0030853398645, 0.00435764496326, 0.00316485893317, 0.00224243773657,
                0.00166990044213, 0.00125640128503, 0.00130411272623, 0.00388053055123, 0.00187665002067, 0.000938325010337,
                0.00209930341296, 0.00144724704984, 0.00248099494259, 0.0012882089125, 0.00151086230478, 0.0017017080696, 0.00124049747129,
                0.00166990044213, 0.00626610261141, 0.00306943605076, 0.000636152549381, 0.00236966824645, 0.00189255383441,
                0.000731575431789, 0.00120868984382, 0.00109736314768, 0.000922421196603, 0.00744298482776, 0.000922421196603,
                3.18076274691e-05, 0.00372149241388, 0.0, 0.0595120709946, 0.0107986895257, 0.0234422214447, 0.0208021883648,
                0.0577308438564, 0.00957409586819, 0.00855625178918, 0.0219472629537, 0.0495880912243, 0.0039123381787, 0.00516873946372,
                0.0372785393937, 0.0138045103216, 0.0333343935876, 0.0260663507109, 0.0245872960336, 0.00116097840262, 0.0317440122141,
                0.0438945259073, 0.0565857692675, 0.0117529183498, 0.0169693692547, 0.00561404624829, 0.00345112758039, 0.0087948089952,
                0.00124049747129, 0.00149495849105, 0.00688635134705, 0.00149495849105, 0.0],[0.0957608959774,
0.00108241843204, 0.0239899268815, 0.00129227506682, 1.10450860412e-05, 3.31352581237e-05, 0.000861516711215,
0.00172303342243, 0.00287172237072, 0.00287172237072, 0.000265082064989, 0.000430758355608, 0.00544522741832,
0.0283196006097, 0.0127681194637, 0.024906669023, 0.00874770814465, 0.00736707238949, 0.00709094523846, 0.00279440676843,
0.00149108661556, 0.00216483686408, 0.00224215246637, 0.00376637434006, 0.00227528772449, 0.00231946806866,
0.00592016611809, 0.00215379177804, 0.0156840221785, 0.0132651483355, 0.0157944730389, 0.000132541032495,
5.52254302061e-05, 0.00261768539177, 0.00188870971305, 0.00218692703616, 0.0043407188142, 0.00222006229429,
0.000751065850803, 0.000596434646226, 0.00300426340321, 0.00103823808787, 0.000574344474143, 0.00299321831717,
0.000773156022885, 0.00216483686408, 0.000960922485586, 0.00207647617575, 0.00298217323113, 0.00132541032495,
0.00143586118536, 0.00586494068789, 0.0025182796174, 0.000463893613731, 0.000883606883298, 0.00150213170161,
1.10450860412e-05, 0.000441803441649, 0.000342397667278, 0.000441803441649, 0.000430758355608, 0.000441803441649, 0.0,
0.00146899644348, 0.0, 0.078773553646, 0.00963131502794, 0.0244096401511, 0.0258234111644, 0.0665024630542,
0.0111886721598, 0.0116415206874, 0.0276237601891, 0.0571583202633, 0.00320307495195, 0.00516910026729, 0.0367470012591,
0.0249729395392, 0.0344496233626, 0.0337758731141, 0.0194614416046, 0.000762110936844, 0.0343281274161, 0.0390333340697,
0.0624710066491, 0.0158386533831, 0.00781992091718, 0.00501446906271, 0.0018224391968, 0.0113101681062, 0.00407563674921,
0.00101614791579, 0.000320307495195, 0.00101614791579, 0.0],[0.0502566222314, 0.00385794495539, 0.0340325858565,
0.000378905308119, 0.0, 0.0019978643519, 0.00103337811305, 0.00626916055251, 0.00337570183597, 0.00337570183597,
0.000241121559712, 0.00141228342117, 0.00671695773484, 0.0333092211774, 0.0158795770039, 0.0322413971272,
0.0125727670421, 0.00843925458992, 0.00919706520616, 0.00320347215046, 0.00310013433915, 0.00334125589887,
0.00213564810031, 0.00444352588612, 0.00113671592436, 0.00151562123248, 0.00974820019979, 0.00289345871654,
0.0212186972547, 0.020667562261, 0.0211153594434, 0.000964486238848, 0.000103337811305, 0.00313458027626,
0.00189452654059, 0.00155006716958, 0.00151562123248, 0.00110226998726, 0.00241121559712, 0.000551134993628,
0.00127449967276, 0.000964486238848, 0.000930040301746, 0.000344459371017, 0.00120560779856, 0.00165340498088,
0.000723364679136, 0.00175674279219, 0.000413351245221, 0.000344459371017, 0.00113671592436, 0.00137783748407,
0.00141228342117, 0.00099893217595, 0.000275567496814, 0.000585580930729, 0.000241121559712, 0.000275567496814,
0.000241121559712, 0.00113671592436, 0.00099893217595, 0.00113671592436, 0.0, 0.00327236402466, 0.0, 0.0504977437911,
0.0080259033447, 0.0265233715683, 0.0276945334298, 0.0699252523165, 0.0099204298853, 0.0126761048534, 0.0188763735317,
0.0514966759671, 0.00496021494265, 0.00682029554614, 0.0336536805484, 0.0256622231408, 0.0421273810754, 0.040990665151,
0.0236643587889, 0.000551134993628, 0.0316902621336, 0.0461920016534, 0.0613482139782, 0.0107126864386, 0.0151562123248,
0.00985153801109, 0.00372016120699, 0.0080947952189, 0.000551134993628, 0.00148117529537, 0.000620026867831,
0.00148117529537, 0.0]]

print  General_Predict (test)
