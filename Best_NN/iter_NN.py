import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import random
import general
import os
import cPickle

def Train (X,Y,lr, k , hidden_neurons, emax , itermax, Beta, lamda, decay):
	Y = Y.reshape (len(Y),1)
	IP1 = np.append(X,np.ones((len(X),1)),1)
	W1 = np.random.rand(hidden_neurons, len(IP1[0])) - 0.5
	W2 = np.random.rand(1,hidden_neurons+1)  - 0.5

	Error_Curve = [] 
	for iter in range (itermax):
		for indx, inst in enumerate(IP1):
			inst = np.array(inst).reshape(len(inst),1)
	# forward propagation first layer
			NET1 = np.dot(W1,inst).transpose() 
       		        Act_1 = np.tanh(k*NET1)#2.0/(1 + np.exp(-k*NET1))-1
        	        F1_prime = k* (1- Act_1*Act_1 )
    	# forward propagation second layer
	                IP2 = np.append(Act_1,np.ones([1,1]),1)
        	        NET2 = np.dot(IP2,  W2.transpose())
        	        Act_2 =  np.tanh(k*NET2) #2.0/(1 + np.exp(-k2*NET2)) -1
        	        F2_prime = k*(1- Act_2*Act_2)
	
	# backward propagation second layer
        	        Error2 = np.subtract(Y[indx], Act_2) 
      		        Delta2 = Error2*F2_prime
	# backward propagation firstlayer
	                Error1 =  np.dot(Delta2,W2)[:, 0:hidden_neurons] # eliminate w0			
      		        Delta1 = Error1*F1_prime

                	DW2 = lr* np.dot(np.transpose(Delta2),IP2)
         	        DW1 = lr* np.dot(inst,Delta1).transpose()

        	        ee = sum (sum ( Error2*Error2)) # compute error
			# update weights

	                if not (indx==0 and iter ==0) :
        	# use momentum to converge faster.
                	        W2 += Beta*DW2 + (1-Beta)*Prev_DW2
                	        W1 += Beta*DW1 + (1-Beta)*Prev_DW1

			else:
				W2 += DW2	
                       		W1 += DW1
			
			Prev_DW1 = DW1 ; Prev_DW2 = DW2 ; Prev_ee = ee 

		'''
	       # Variable Learning Rat
	                        inc = 100.0*(ee - Prev_ee)/Prev_ee
        	                if inc >= 5.0:
                	                lr *= (1.0 / (1.0 + decay*iter))
                	                inc *= -1
          		        if inc >= 5.0:
                      		         lr *=  (1.0 + decay*iter)

		'''
		Error_Curve.append (ee)
		if ee < emax:
			break
		
	return W1,W2,Error_Curve

def predict ( test,W1,W2,k):

        IP1 = np.append(test,np.ones([len(test),1]),1)
        NET1 =  np.dot(IP1, W1.transpose() )
        Activation_1 = np.tanh(k*NET1)#2.0/(1 + np.exp(-k1*NET1)) -1

        IP2 = np.append(Activation_1,np.ones([len(Activation_1),1]),1)
        NET2 = np.dot(IP2,  W2.transpose())
        output= np.tanh(k*NET2)#2.0/(1 + np.exp(-k2*NET2)) - 2

        return output


def K_Folds (K, X,Y,lr,k,hidden_neurons,emax,itermax,beta,lamda,decay ):
#       print "K fold validation with K= %d"%K
        Range = len(X)//K
        cnt = 1 ;
        GRNN_acc = 0;	FP = 0
        FN = 0;	TP = 0 ; TN = 0

        for indx in range (0,len(X) , Range):
                print "Test fold num %d "%(cnt),
		# create your folds. 
                test_D = np.array(X[indx:indx+Range])
                test_L =  np.array(Y[indx:indx+Range])
                train_D = np.concatenate (( X[0:indx], X[indx+Range:len(X)] ))
                train_L =np.concatenate ((Y[0:indx],Y[indx+Range:len(Y)]))
		# train
		print "testing data has pos: ",np.count_nonzero(test_L==1), " neg: ",np.count_nonzero(test_L== -1) 
                W1,W2,Error_Curve = Train(train_D,train_L,lr,k,hidden_neurons,emax,itermax,beta,lamda,decay)
#		print 
#		print "Erorr Curve is as follows: ", Error_Curve
#		print
		# predict & evaluate 
#		output = predict (train_D,W1,W2,k)
#               acc, fp, fn, tp , tn  = general.Accuracy(output,train_L)
#		print "Training Accuracy:%f FP:%d FN:%d TP:%d TN:%d"%(acc,fp,fn,tp,tn)

                output = predict (test_D,W1,W2,k)        
	        acc, fp, fn, tp , tn  = general.Accuracy(output,test_L)
		print "Testing Accuracy:%f FP:%d FN:%d TP:%d TN:%d"%(acc,fp,fn,tp,tn)
                FP += fp        ; FN += fn;       GRNN_acc += acc;
                TP += tp ; TN += tn
                cnt+=1
                print

        #print "Accuracy: %f  FP: %d , FN: %d, TP: %d , TN %d"%(GRNN_acc/K,FP,FN,TP,TN)
        return GRNN_acc/K, FP, FN, TP , TN


def   General_Predict (test):
	with open (os.path.join(os.path.dirname(__file__), "W1"),"rb") as file:
                W1 = cPickle.load (file)

        with open (os.path.join(os.path.dirname(__file__), "W2"),"rb") as file:
                W2 = cPickle.load (file)

        k =1 ; 

	test = np.array(general.standarize (test) ) 
	output = predict (test,W1,W2,k)
	return output 


def Main ():
	X,Y = general.Load_Data("malware_dataset.csv")
	X,Y = general.Balance_Data (X,Y)
        X,Y = general.shuffle_in_unison_scary (X,Y)
#        X = general.Normalization_min_max (X)
	X = general.standarize(X)
	# parameters 
	lr = 0.01423 ; hidden_neurons = 1 ; emax = 0.001 ; K = 3 #len(X)
	beta = 0.815432 ; decay = 0.0001 ; k = 1;  lamda = 1 ; itermax = 100
#	lr = 0.001
	acc, FP,FN,TP,TN = K_Folds(K, X,Y,lr,k,hidden_neurons,emax,itermax,beta,lamda,decay )

	print "\nFinal: acc: %f FP: %d FN: %d TP: %d TN: %d"%(acc, FP,FN,TP,TN) 
	print 
     

	W1,W2,Error_Curve = Train(X,Y,lr,k,hidden_neurons,emax,itermax,beta, lamda,decay)
	
	print "\nstoring NN Model: "

        with open ("Params", "wb") as file:
                cPickle.dump ([lr,k,hidden_neurons,emax,itermax,beta, lamda,decay],file)

        with open ("W1", "wb")as file:
                cPickle.dump (W1,file)

        with open ("W2", "wb")as file:
                cPickle.dump (W2,file)
        print "DONE ! "

