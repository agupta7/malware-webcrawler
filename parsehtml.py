import numpy as np

ASCII = []
FV = []
def ASCII_bounded (cls , url , contents):
## ASCII FROM 32 to  126 are included.
        global ASCII
        ASCII = np.array([ord(char) for char in contents]) # get ascci for every char in the html file.

        delete = np.where (ASCII < 32)  # determine the undesired
        ASCII = np.delete(ASCII, delete)        # delete
        delete = np.where(ASCII > 126)  # undesired greater than 126
        ASCII = np.delete(ASCII, delete)

import collections
def buildUnigram (cls , url):
        global FV
        # use the ascii list of the html page to build the unigrame feature vector.
        Counter = collections.Counter (ASCII) # counter for every eleemnt in the list
        char_num = len(ASCII)

        for count in range(32, 127):            # Feature vector -->  frequency of each char divided by total # chars
                FV.append( Counter[count]/float(char_num))

def extractLinks (url, contents):
        return 0
def parseHTML(cls, url, contents):
        ASCII_bounded(cls , url , contents)
#  html text is now held in global list ASCII  of all html chars between 32 & 126
        buildUnigram(cls , url)
        extractLinks(url, contents)
