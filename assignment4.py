#!/usr/bin/python -uB
""" Assignment 4 for malware-webcrawler """
import json
import sys
import pickle
import math
import util
import numpy as np
from WebCrawller import WebCrawller
from GRNN_latest.GRNN import GRNNPredictor

def main(datasetFile, rootURLs):
    """ Main function of assignment 4 """
    for rootURL in rootURLs:
        unigramList = None
        print "----------- {0} ------------".format(rootURL)
        if '.unigramList' in rootURL:
            unigramList = pickle.load(open(rootURL, 'rb'))
            print "Loaded {0} unigrams from file.".format(len(unigramList))
        elif '.unigram' in rootURL:
            unigramList = util.treeToList(pickle.load(open(rootURL, 'rb')))
            print "Loaded {0} unigrams from file.".format(len(unigramList))
        else:
            unigramTree = json.loads(WebCrawller.crawlIDS(rootURL, 1)['json'])
            unigramList = util.treeToList(unigramTree)
            pickle.dump(unigramList, open('unigrams/url.unigram', 'wb'))
            print "Saved {0} unigrams to unigrams/url.unigram".format(len(unigramList))

        n = 200
        predictions = findURLsInRange(datasetFile, unigramList, -0.015, 0.015)[:n]
        mean = float(sum(predictions)) / len(predictions)
        if len(predictions) >= n:
            print "mean: {0}     deviation: {1}".format(mean, math.sqrt(sum((mean - np.array(predictions)) **2) / len(predictions)))

            mean_1to100 = float(sum(predictions[:100])) / 100
            print "First 100 :           mean: {0}     deviation: {1}".format(mean_1to100, math.sqrt(sum((mean_1to100 - np.array(predictions[:100])) ** 2) / 100))

            mean_101to200 = float(sum(predictions[100:200])) / 100
            print "Second 100 :           mean: {0}     deviation: {1}".format(mean_101to200, math.sqrt(sum((mean_101to200 - np.array(predictions[100:200])) ** 2) / 100))
        else:
            print "mean: {0}     deviation: {1}".format(mean, math.sqrt(sum((mean - np.array(predictions)) ** 2) / len(predictions)))
            print >> sys.stderr, "Only {0} unigrams for URL {1}!\n".format(len(predictions), rootURL)

def findURLsInRange(dataset, unigramList, lower, upper):
    """
        unigramList is a list from the output of WebCrawler.crawlIDS function.
        Each member contains the following properties:
            'url': The subject URL
            'unigram': The feature vector in question
    """
    idealSigma = 0.246711580707
    grnn = GRNNPredictor(dataset, idealSigma)

    predictions = []
    for link in unigramList:
        prediction = grnn.predict(link['unigram'])
        predictions.append(prediction)
        if prediction > lower and prediction <= upper:
            print "URL {0} has a prediction of {1} with unigram {2}".format(link['url'], prediction, '''link['unigram']''')
        else:
            print "nomatch : URL {0} has a prediction of {1}".format(link['url'], prediction)

    return predictions


if __name__ == "__main__":
    if len(sys.argv) < 3:
        print >> sys.stderr, "Usage: {0} dataset_file rootURLs...".format(sys.argv[0])
        sys.exit()
    else:
        main(sys.argv[1], sys.argv[2:])
