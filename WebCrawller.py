from WebCrawllerPage import WebCrawllerPage, UnsupportedTypeException
from FeatureExtractorUnigram import FeatureExtractorUnigram
from urllib2 import HTTPError, URLError
import sys
import time
'''
 1. Instantiate node with just URL
 2. Find all descendent links and instantiate those
 3. Convert toJson
        run feature extractor on node
        loop children recursively
        output json
'''
class WebCrawller:
    @classmethod
    def crawlIDS(cls, rootUrl, maxDepth):
        rootPage = None
        depthLimit = 0
        data = None

        # start at depth of 0 and iterate DFS to maxdepth
        while (depthLimit <= maxDepth):
            Dict = set()
            data = cls.crawlDepthFirstSearch(WebCrawllerPage(rootUrl), depthLimit, Dict)
            depthLimit = depthLimit + 1

        print 'Urls successfully analyzed: ' + str(data['urls'])
        print 'Time taken (milliseconds): ' + str(data['time'])
        print 'Links avoid due to cycles: ' + str(data['avoided'])
        print 'Problem links: ' + str(data['errors'])
        print 'Sum of unigram: ' + str(data['unigramSum'])
        return data['json']

    @classmethod
    def crawlDepthFirstSearch(cls, webCrawllerPage, depthLimit, Dict):
        output = {'json':'', 'time': 0, 'urls': 0, 'errors': 0, 'unigramSum': 0, 'avoided': 0}
        startTime = time.time()
        try:
            json = '{"url": "' + webCrawllerPage.url + '"'

            unigram = FeatureExtractorUnigram.extractFeatures(webCrawllerPage.getFileStream(), webCrawllerPage.encoding)
            json = json + ', "unigram": [' + ', '.join(str(feature) for feature in unigram) + ']'

            json = json + ', "links": ['

            if webCrawllerPage.url in Dict:
                raise Cycle(webCrawllerPage.url)

            Dict.add(webCrawllerPage.url)

            if depthLimit > 0:
                # extractLinks() gets array of strings
                links = webCrawllerPage.extractLinks()
                for index, link in enumerate(links):
                    # turn it into an array of objects
                    childPage = WebCrawllerPage(link)
                    webCrawllerPage.links.append(childPage)

                    childData = cls.crawlDepthFirstSearch(childPage, depthLimit - 1, Dict)
                    if (childData['json'] != ''):
                        json = json + childData['json'] + ','
                    output['errors'] = output['errors'] + childData['errors']
                    output['avoided'] = output['avoided'] + childData['avoided']
                    output['urls'] = output['urls'] + childData['urls']
                
                # there was an extra comma added to the end of the links array in the last iteration.  Remove it:
                if json[-1:] == ',':
                    json = json[:-1]

            json = json + ']}'

            output['json'] = json
            output['time'] = int((time.time() - startTime) * 1000)
            output['urls'] = output['urls'] + 1
            for charFrequency in unigram:
                output['unigramSum'] = output['unigramSum'] + float(charFrequency)
            return output 
        
        except Cycle as e: # handle the cycle error
            print 'Referring to node ' + e.value + ' creates a cycle and is not allowed'
            output['json'] = ''
            output['avoided'] = output['avoided'] + 1
            return output
        except UnsupportedTypeException as e:
            print e
            output['json'] = ''
            output['errors'] = output['errors'] + 1
            return output
        except HTTPError as e:
            print 'HTTP error ' + str(e.code) + ' ' + e.reason + ' for url ' + webCrawllerPage.url
            output['json'] = ''
            output['errors'] = output['errors'] + 1
            return output
        except URLError as e:
            print 'Server not found for url ' + webCrawllerPage.url
            output['json'] = ''
            output['errors'] = output['errors'] + 1
            return output
        except Exception as e:
            print >> sys.stderr, e.message + ' exception in URL ' + webCrawllerPage.url
            print >> sys.stderr, e
            output['json'] = ''
            output['errors'] = output['errors'] + 1
            return output

class Cycle (Exception):
        def __init__(self, value):
                self.value = value
        def __str__(self):
                return repr(self.value)

