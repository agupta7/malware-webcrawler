class WebCrawller:
    @classmethod
    def crawlIDS(cls, rootUrl, maxDepth):
        rootPage = None
        depthLimit = 0
        while (depthLimit <= maxDepth):
            rootPage = crawlDepthFirst(WebCrawllerPage(rootUrl), depthLimit++)
        return rootPage.toJson(true)
    @classmethod
    def crawlDepthFirst(cls, webCrawllerPage, depthLimit):
        url = webCrawllerPage.url

        contents = downloadUrl(url)
        saveFile(url, contents)
        
        if depthLimit > 0:
            links = extractLinks(url, contents)
            for link in links:
                child = WebCrawllerPage(link)
                crawlDepthFirst(child, depthLimit - 1)
                webCrawllerPage.descendants.append(child)

        return webCrawllerPage

    @classmethod
    def downloadUrl(cls, url):
        # check for HTTP header 'Content-Type: text/html'
        # bail if not text/html
        pass
    @classmethod
    def saveFile(cls, url, urlContents):
        # do this part last
        # we might want to extract this method out to a new class/utility module
        pass
    


    @classmethod
    def extractLinks(cls, url, contents):
        # what about links that occur in HTML comments, script tags, and link tags
        # swati
        pass
    @classmethod
    def buildUnigram(cls, url, contents):
        # might want to extract this method out to a more generic class
        # could be responsible for building generic feature vectors out of input
        # swati
        pass
    @classmethod
    def parseHTML(cls, url, contents):
        # what to do if HTML doc uses Unicode characters beyond the ASCII range     
        extractLinks(url, contents)
        buildUnigram(url, contents)
        # ...
        pass
