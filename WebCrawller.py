import urllib2
from bs4 import BeautifulSoup, Comment
import numpy as np


#Global Variables
hrefs = []
ASCII = []
text = ""
contents = []

class WebCrawller:
    
    @classmethod
    def crawl(cls, rootUrl, depthLimit):
        return "{\"" + rootUrl + "\": {\"unigram\": \"feature vector / bitmap\", \"descendents\": [\"array of objects with same structure\"]}}"
    
    @classmethod
    def downloadUrl(cls, url):
        # check for HTTP header 'Content-Type: text/html'
        # bail if not text/html
        response = urllib2.urlopen(url)

        html_contents = response.read().decode('utf-8')
        #swati - for my testing
        #print html_contents

        instance=cls()
        instance.extractLinks(html_contents)
        instance.extractChars(html_contents)
        instance.ASCII_bounded()
        pass
    
    @classmethod
    def saveFile(cls, url, urlContents):
        # do this part last
        # we might want to extract this method out to a new class/utility module
        pass

    @classmethod
    def scrapeHTML(cls, html_contents):
        # what about links that occur in HTML comments, script tags, and link tags
        # swati
        soup = BeautifulSoup(html_contents, 'html.parser')

        for script in soup(["script", "style"]):
            script.extract()
        comments = soup.findAll(text=lambda text:isinstance(text, Comment))
        [comment.extract() for comment in comments]
        #swati - for my testing
        #print soup
        return soup
    
    @classmethod
    def extractLinks(cls, html_contents):
        instance=cls()
        soup = instance.scrapeHTML(html_contents)
        links = soup.find_all('a')
        global hrefs
        for link in soup.find_all('a'):
            #swati - for my testing
            #print(link.get('href'))
            hrefs.append(link.get('href'))
            #print hrefs
        #swati - for my testing
        #print type(links)
        #TODO: sanitize href to get rid of blank links and links to a section of the same page 
        pass

    @classmethod
    def extractChars(cls, html_contents):
        instance=cls()
        soup = instance.scrapeHTML(html_contents)
        global text
        text = soup.get_text()
        global contents 
        contents = list(text)
        #swati - for my testing
        #print contents
        #TODO: Convert text to an array of characters - contents
        #return contents

    @classmethod
    def ASCII_bounded (cls):
    ## ASCII FROM 32 to  126 are included.
        global ASCII
        global contents
        
        ASCII = np.array([ord(char) for char in contents]) # get ascci for every char in the html file.
        
        delete = np.where (ASCII < 32)  # determine the undesired
        ASCII = np.delete(ASCII, delete)        # delete

        delete = np.where(ASCII > 126)  # undesired greater than 126
        ASCII = np.delete(ASCII, delete)
        
        #swati - for my testing
        #print ASCII
        #return ASCII

    @classmethod
    def buildUnigram(cls, url, contents):
        # might want to extract this method out to a more generic class
        # could be responsible for building generic feature vectors out of input
        # swati
        pass

    @classmethod
    def parseHTML(cls, url, contents):
        # what to do if HTML doc uses Unicode characters beyond the ASCII range     
        extractLinks(url, contents)
        buildUnigram(url, contents)
        # ...
        pass
