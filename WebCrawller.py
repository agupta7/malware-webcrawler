import urllib2
from bs4 import BeautifulSoup, Comment
import numpy as np

class WebCrawller:

    # List of extracted links from an input url
    hrefs = []
    # Array of characters of all unicode characters in a web page
    contents = []
    # Array of characters of printable ascii characters in a web page
    ASCII = []
    
    @classmethod
    def crawl(cls, rootUrl, depthLimit):
        return "{\"" + rootUrl + "\": {\"unigram\": \"feature vector / bitmap\", \"descendents\": [\"array of objects with same structure\"]}}"
    
    @classmethod
    def downloadUrl(cls, url):
        # TODO: check for HTTP header 'Content-Type: text/html'
        # bail if not text/html
        
        #Expand the input url
        response = urllib2.urlopen(url)

        #Extract the HTML out of input URL
        html_contents = response.read().decode('utf-8')
        #swati - for my testing
        #print html_contents

        instance=cls()
        
        # Extracting all the links from the HTML contents
        hrefs = instance.extractLinks(html_contents)

        # Extracting all the unicode characters from the HTML contents
        contents = instance.extractChars(html_contents)

        # Extracting all the printable characters from the HTML contents
        instance.ASCII_bounded(contents)
        pass
    
    @classmethod
    def saveFile(cls, url, urlContents):
        # do this part last
        # we might want to extract this method out to a new class/utility module
        pass

    # Crate a BeautifulSoup object called soup with all objectified HTML
    @classmethod
    def soupHTML(cls, html_contents):
        # HTML tags in soup now represent Python objects
        soup = BeautifulSoup(html_contents, 'html.parser')
        return soup

    #Cleans HTML of all the script tags, style tags and comments i.e. clean HTML
    @classmethod
    def scrapeHTML(cls, html_contents):
        instance=cls()
        soup = instance.soupHTML(html_contents)
        # Removing out script and style tags/objects from soup
        for script in soup(["script", "style"]):
            script.extract()
        
        # Removing out all the Comment objects from soup
        comments = soup.findAll(text=lambda text:isinstance(text, Comment))
        [comment.extract() for comment in comments]
        
        #swati - for my testing
        #print soup
        return soup
    
    @classmethod
    def extractLinks(cls, html_contents):
        
        # Calling soupHTML to get unclean objectified HTML
        instance=cls()
        soup = instance.soupHTML(html_contents)

        # # Calling scrapeHTML to get clean objectified HTML
        # instance=cls()
        # soup = instance.scrapeHTML(html_contents)
        
        # Extract link objects from objectified HTML
        links = soup.find_all('a')

        # List of all links in for a given URL
        hrefs = []
        
        # Extract the href values from links and make a list of them called href 
        for link in links:
            #swati - for my testing
            #print(link.get('href'))
            hrefs.append(link.get('href'))
            #print hrefs
        #swati - for my testing
        #print type(links)
        
        # Extract Non blank links from hrefs
        hrefs = list( filter((lambda x: x != None), hrefs))
        
        # Extract # links from hrefs 
        #TODO: Not sure if this is the correct way to remove references to the same page
        hrefs = list( filter((lambda x: x.find('#') == -1), hrefs))
        
        #swati - for my testing
        #print hrefs
        return hrefs

    @classmethod
    def extractChars(cls, html_contents):
        # Calling soupHTML to get unclean objectified HTML
        instance=cls()
        soup = instance.soupHTML(html_contents)

        # # Calling scrapeHTML to get clean objectified HTML
        # instance=cls()
        # soup = instance.scrapeHTML(html_contents)
        
        # Getting the text out the soup object which has objectified HTML
        text = soup.get_text()
        # Convering HTML text to a list of unicode characters
        contents = list(text)
        
        #swati - for my testing
        #print contents
        return contents

    @classmethod
    def ASCII_bounded (cls, contents):
    ## ASCII FROM 32 to  126 are included.
        
        ASCII = np.array([ord(char) for char in contents]) # get ascci for every char in the html file.
        
        delete = np.where (ASCII < 32)  # determine the undesired
        ASCII = np.delete(ASCII, delete)        # delete

        delete = np.where(ASCII > 126)  # undesired greater than 126
        ASCII = np.delete(ASCII, delete)
        
        #swati - for my testing
        #print ASCII
        return ASCII

    @classmethod
    def buildUnigram(cls, url, contents):
        # might want to extract this method out to a more generic class
        # could be responsible for building generic feature vectors out of input
        pass

    @classmethod
    def parseHTML(cls, url, contents):
        # what to do if HTML doc uses Unicode characters beyond the ASCII range     
        extractLinks(url, contents)
        buildUnigram(url, contents)
        # ...
        pass
