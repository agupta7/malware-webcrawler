from WebCrawllerPage import WebCrawllerPage
from FeatureExtractorUnigram import FeatureExtractorUnigram
'''
 1. Instantiate node with just URL
 2. Find all descendent links and instantiate those
 3. Convert toJson
        run feature extractor on node
        loop children recursively
        output json
'''
class WebCrawller:
    @classmethod
    def crawlIDS(cls, rootUrl, maxDepth):
        rootPage = None
        depthLimit = 0
        json = ''

        # start at depth of 0 and iterate DFS to maxdepth
        while (depthLimit <= maxDepth):
            json = cls.crawlDepthFirstSearch(WebCrawllerPage(rootUrl), depthLimit)
            depthLimit = depthLimit + 1
        return json

    @classmethod
    def crawlDepthFirstSearch(cls, webCrawllerPage, depthLimit):
        try:
            json = '{"url": "' + webCrawllerPage.url + '"'

            unigram = FeatureExtractorUnigram.extractFeatures(webCrawllerPage.getFileStream(), webCrawllerPage.encoding)
            json = json + ', "unigram": ' + ', '.join(str(feature) for feature in unigram)

            json = json + ', links: ['
            if depthLimit > 0:
                # extractLinks() gets array of strings
                links = webCrawllerPage.extractLinks()
                for index, link in enumerate(links):
                    # turn it into an array of objects
                    childPage = WebCrawllerPage(link)
                    webCrawllerPage.links.append(childPage)
                    json = json + (',' if index == 0 else '') + cls.crawlDepthFirstSearch(childPage, depthLimit - 1)
            json = json + ']}'
            return json
        except ZeroDivisionError as e:
            print(e)
            return ''

