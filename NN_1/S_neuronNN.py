import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import random
import general
import cPickle 
import unigrames
import unigrames_2
import sys
import math

def Train (X,Y,lr, k , hidden_neurons, emax , itermax, Beta, lamda, der):

	Y = Y.reshape (len(Y),1)
	IP1 = np.append(X,np.ones((len(X),1)),1)
	W1 = np.random.rand(hidden_neurons, len(IP1[0])) - 0.5
	W2 = np.random.rand(1,hidden_neurons+1)  - 0.5

	Error_Curve = [] 
	for iter in range (itermax):
		for indx, inst in enumerate(IP1):
			inst = np.array(inst).reshape(len(inst),1)
	# forward propagation first layer
			NET1 = np.dot(W1,inst).transpose() 
       		        Act_1 = np.tanh(k*NET1)+ der*NET1
			#Act_1 = 2.0/(1 + np.exp(-k*NET1))-1
			F1_prime = k* (1- Act_1*Act_1 )

	# backward propagation firstlayer
	                Error1 =  np.subtract(Y[indx], Act_1)
#			Error1 += 0.5*lamda*np.dot(W1,W1.transpose()) # regularization

      		        Delta1 = Error1*F1_prime
         	        DW1 = lr* np.dot(inst,Delta1).transpose()

        	        ee = sum (sum ( Error1*Error1)) # compute error
			# update weights

	                if not (indx==0 and iter ==0) :
        # use momentum to converge faster.
                	        W1 += Beta*DW1 + (1-Beta)*Prev_DW1
			else:
	                  	W1 += DW1
			
			Prev_DW1 = DW1 ; 

		if ee < emax:
			break 		

		Error_Curve.append (ee)

	return W1,W2,Error_Curve

def predict ( test,W1,W2,k, der):
        IP1 = np.append(test,np.ones([len(test),1]),1)
        NET1 =  np.dot(IP1, W1.transpose() )
        output = np.tanh(k*NET1) + der*NET1
#	output = 2.0/(1 + np.exp(-k*NET1)) -1

        return output

