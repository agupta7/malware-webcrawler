import numpy as np
from bs4 import BeautifulSoup, Comment
import urllib2
import StringIO
from util import *
from urlparse import urljoin

class WebCrawllerPage:
    def __init__(self, url):
        self.url = url
        self.links = []

        #initializing these to None
        self.encoding = None

    def extractLinks(self):
        #contents = self.getFileStream().read().decode(self.encoding)
        htmlSoup = BeautifulSoup(self.getFileStream, 'html.parser')

        links = htmlSoup.find_all('a', href=True)
        hrefs = []
        for link in links:
            url = link.get('href')
            if '#' in url:
                url = url[::url.index('#')-1]
            if validateUrl(url):
                hrefs.append(url) 
            if validateUrl(urljoin(self.url, url)):
                hrefs.append(urljoin(self.url, url))

        # filter out blank links
        hrefs = list(filter((lambda x: x != None or x != ''), hrefs))
        
        # TODO : what to do with links targeting within same page '...#anchortag'
        return hrefs

    '''
        Open a stream from on-disk file cache if it exists.
        Otherwise get stream from HTTP client
    '''
    def getFileStream(self):
        ''' 1. Request Headers:
                Accept: text/html only
                Accept encoding: utf-8 preferred
            2. Send GET
            3. Reject response if not text/html.  Return an empty file handle
        '''
        # TODO : cache URL data in file
        # TODO : Use modified stamp and etags to figure out if cache should be updated
        emptyStream = StringIO.StringIO('')
        self.encoding = 'utf-8'
        try:
            opener = urllib2.build_opener()
            opener.addheaders[('Accept', 'text/html')]
            urlstream = opener(self.url)
            self.encoding = urlstream.headers.getparam('charset') or self.encoding
            return urlstream
        except Exception as e:
            print(e)
            return emptyStream

