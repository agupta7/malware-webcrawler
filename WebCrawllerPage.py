import numpy as np
from bs4 import BeautifulSoup, Comment
import urllib2
from util import *
from urlparse import urljoin

class WebCrawllerPage:
    def __init__(self, url):
        self.url = url
        self.links = []

        #initializing these to None
        self.encoding = None

    def extractLinks(self):
        htmlSoup = BeautifulSoup(self.getFileStream(), 'html.parser')

        links = htmlSoup.find_all('a', href=True)
        hrefs = []
        for link in links:
            url = link.get('href')
            if '#' in url:
                url = url[:url.index('#')]
            if validateUrl(url):
                hrefs.append(url) 
            if validateUrl(urljoin(self.url, url)):
                hrefs.append(urljoin(self.url, url))

        # filter out blank links
        hrefs = set(filter((lambda x: x != None or x != ''), hrefs))
        
        return hrefs

    '''
        Open a stream from on-disk file cache if it exists.
        Otherwise get stream from HTTP client
    '''
    def getFileStream(self):
        ''' 1. Request Headers:
                Accept: text/html only
                Accept encoding: utf-8 preferred
            2. Send GET
            3. Reject response if not text/html.  Return an empty file handle
        '''
        # TODO : cache URL data in file
        # TODO : Use modified stamp and etags to figure out if cache should be updated
        self.encoding = 'utf-8'
        # https://stackoverflow.com/questions/13303449/urllib2-httperror-http-error-403-forbidden
        hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',
                       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                      'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
                     'Accept-Encoding': 'none',
                        'Accept-Language': 'en-US,en;q=0.8',
                       'Connection': 'keep-alive'}

        req = urllib2.Request(self.url, headers=hdr)
        urlstream = urllib2.urlopen(req)
        http_headers = urlstream.info()
        self.encoding = http_headers.getparam('charset') or self.encoding

        if http_headers.type != 'text/html':
            raise UnsupportedTypeException('skipping ' + self.url + ' because its Content-Type is ' + http_headers.type)
        return urlstream

class UnsupportedTypeException(Exception):
    pass
