import numpy as np
from bs4 import BeautifulSoup, Comment
import urllib2
from util import *
from urlparse import urljoin

class WebCrawllerPage:
    def __init__(self, url):
        self.url = url
        self.links = []

        #initializing these to None
        self.encoding = None

    def extractLinks(self):
        htmlSoup = BeautifulSoup(self.getFileStream(), 'html.parser')

        links = htmlSoup.find_all('a', href=True)
        hrefs = []
        for link in links:
            url = link.get('href')
            if '#' in url:
                url = url[::url.index('#')-1]
            if validateUrl(url):
                hrefs.append(url) 
            if validateUrl(urljoin(self.url, url)):
                hrefs.append(urljoin(self.url, url))

        # filter out blank links
        hrefs = list(filter((lambda x: x != None or x != ''), hrefs))
        
        # TODO : what to do with links targeting within same page '...#anchortag'
        return hrefs

    '''
        Open a stream from on-disk file cache if it exists.
        Otherwise get stream from HTTP client
    '''
    def getFileStream(self):
        ''' 1. Request Headers:
                Accept: text/html only
                Accept encoding: utf-8 preferred
            2. Send GET
            3. Reject response if not text/html.  Return an empty file handle
        '''
        # TODO : cache URL data in file
        # TODO : Use modified stamp and etags to figure out if cache should be updated
        self.encoding = 'utf-8'
        opener = urllib2.build_opener()
        opener.addheaders = [('Accept', 'text/html')]
        urlstream = opener.open(self.url)
        self.encoding = urlstream.headers.getparam('charset') or self.encoding
        return urlstream

